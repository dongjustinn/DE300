{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd573a76-8490-45cd-9db2-5e063bd9b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 33.2M  100 33.2M    0     0   218M      0 --:--:-- --:--:-- --:--:--  219M\n"
     ]
    }
   ],
   "source": [
    "# Create a folder called dataset\n",
    "!mkdir -p dataset\n",
    "\n",
    "# Download the cleaned AG news file\n",
    "!curl -o dataset/agnews_clean.csv https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb35692-6f2b-41b4-b796-3731ede9cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession to run PySpark commands\n",
    "# the .master() command tells Spark to run locally\n",
    "# the .appName() names it AG news\n",
    "# the .getOrCreate() starts a new session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"AG news\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5443dc-0218-4c51-a59f-b4fdf23079ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|_c0|                      filtered|\n",
      "+---+------------------------------+\n",
      "|  0|[wall, st, bears, claw, bac...|\n",
      "|  1|[carlyle, looks, toward, co...|\n",
      "|  2|[oil, economy, cloud, stock...|\n",
      "|  3|[iraq, halts, oil, exports,...|\n",
      "|  4|[oil, prices, soar, time, r...|\n",
      "+---+------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:09:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the AG News dataset into a Spark data frame\n",
    "agnews = spark.read.csv(\"dataset/agnews_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Import F and ArrayType and StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# The filtered column is currently a string that looks like a list. \n",
    "# This converts it into an actual array of strings to work with individual words\n",
    "agnews = agnews.withColumn(\"filtered\", F.from_json(\"filtered\", ArrayType(StringType())))\n",
    "\n",
    "# Show the first 5 rows to see if everything looks okay\n",
    "agnews.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e00b850-b37d-4e98-b629-78984a88ccaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', 'filtered']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agnews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db814a49-facb-4b41-8bb4-8446eb883255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     word|\n",
      "+---+---------+\n",
      "|  0|     wall|\n",
      "|  0|       st|\n",
      "|  0|    bears|\n",
      "|  0|     claw|\n",
      "|  0|     back|\n",
      "|  0|    black|\n",
      "|  0|  reuters|\n",
      "|  0|  reuters|\n",
      "|  0|    short|\n",
      "|  0|  sellers|\n",
      "|  0|     wall|\n",
      "|  0|   street|\n",
      "|  0|dwindling|\n",
      "|  0|     band|\n",
      "|  0|    ultra|\n",
      "|  0|   cynics|\n",
      "|  0|   seeing|\n",
      "|  0|    green|\n",
      "|  1|  carlyle|\n",
      "|  1|    looks|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:36:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Break apart each array of words in the filtered column so each word gets its own row\n",
    "# I create a new column called word with one word per row per document\n",
    "exploded = agnews.withColumn(\"word\", F.explode(\"filtered\"))\n",
    "\n",
    "# Rename the default column name \"_c0\" to \"id\"\n",
    "exploded = exploded.withColumnRenamed(\"_c0\", \"id\")\n",
    "\n",
    "# Show the first 20 rows to check that there is one word per row and an id for each doc\n",
    "exploded.select(\"id\", \"word\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35da3536-774d-46e0-be1f-2c45a142bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:11:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "25/05/24 22:11:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "[Stage 12:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|      word|                  tf|\n",
      "+---+----------+--------------------+\n",
      "| 10|    stocks|                0.04|\n",
      "| 21|    nation| 0.05555555555555555|\n",
      "| 36|      news| 0.07692307692307693|\n",
      "| 44|     salem| 0.02631578947368421|\n",
      "| 48|government|0.047619047619047616|\n",
      "+---+----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Count how many times each word appears in each document\n",
    "word_counts = exploded.groupBy(\"id\", \"word\").count().withColumnRenamed(\"count\", \"word_count\")\n",
    "\n",
    "# Count how many total words are in each document\n",
    "total_words = exploded.groupBy(\"id\").count().withColumnRenamed(\"count\", \"total_words\")\n",
    "\n",
    "# Join the word counts and total word counts on the document ID\n",
    "tf = word_counts.join(total_words, on=\"id\")\n",
    "\n",
    "# Calculate tf by dividing word count by the total words\n",
    "tf = tf.withColumn(\"tf\", F.col(\"word_count\") / F.col(\"total_words\"))\n",
    "\n",
    "# Check the result to see if it looks alright\n",
    "tf.select(\"id\", \"word\", \"tf\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfba2c94-4513-465b-8b14-750d427b2410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:11:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "[Stage 18:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     word|               idf|\n",
      "+---------+------------------+\n",
      "|    still|4.0242864276084385|\n",
      "|arguments| 7.245796143375976|\n",
      "|   doubts| 6.609161173079373|\n",
      "|   online|3.9552643296013406|\n",
      "|   filing| 5.930655542512376|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Count how many documents each word appears in\n",
    "# Duplicates are dropped so each document and word pair is unique\n",
    "# The document frequency is counted by how many documents each word shows up in\n",
    "doc_freq = exploded.select(\"id\", \"word\").distinct().groupBy(\"word\").count().withColumnRenamed(\"count\", \"df\")\n",
    "\n",
    "# Get the total number of documents in the dataset\n",
    "num_docs = agnews.count()\n",
    "\n",
    "# Calculate the inverse document frequency\n",
    "# The log of the total documents divided by document frequency\n",
    "idf = doc_freq.withColumn(\"idf\", F.log(F.lit(num_docs) / F.col(\"df\")))\n",
    "\n",
    "# Preview the IDF values\n",
    "idf.select(\"word\", \"idf\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9dccf7-12bf-40c4-90b3-63fd1bc7d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:13:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "25/05/24 22:13:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "25/05/24 22:13:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "[Stage 69:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------------+\n",
      "|id |word         |tf_idf             |\n",
      "+---+-------------+-------------------+\n",
      "|0  |wall         |0.5115985326511431 |\n",
      "|0  |cynics       |0.563734318747707  |\n",
      "|0  |green        |0.2877107940095433 |\n",
      "|0  |ultra        |0.4125512394225831 |\n",
      "|0  |claw         |0.499114829314058  |\n",
      "|0  |back         |0.1892216338539946 |\n",
      "|0  |st           |0.2584728642725166 |\n",
      "|0  |sellers      |0.4468379768438066 |\n",
      "|0  |dwindling    |0.4572386180709258 |\n",
      "|0  |band         |0.3643421454792778 |\n",
      "|0  |reuters      |0.24754017186645658|\n",
      "|0  |bears        |0.3372044607529448 |\n",
      "|0  |black        |0.2953171727366614 |\n",
      "|0  |short        |0.2773120373951269 |\n",
      "|0  |seeing       |0.37743394553516213|\n",
      "|0  |street       |0.24678348986493034|\n",
      "|1  |industry     |0.15043731768548949|\n",
      "|1  |aerospace    |0.2581171817448437 |\n",
      "|1  |toward       |0.1898997183872362 |\n",
      "|1  |carlyle      |0.7168306746824437 |\n",
      "|1  |timed        |0.324478643568105  |\n",
      "|1  |investment   |0.1890771769001148 |\n",
      "|1  |commercial   |0.2057832028092643 |\n",
      "|1  |reputation   |0.2578098186776328 |\n",
      "|1  |group        |0.12468100563149095|\n",
      "|1  |plays        |0.22418048797172685|\n",
      "|1  |reuters      |0.1650267812443044 |\n",
      "|1  |firm         |0.15969712503706046|\n",
      "|1  |private      |0.1929050573011279 |\n",
      "|1  |market       |0.13394932212703356|\n",
      "|1  |occasionally |0.33274321954270536|\n",
      "|1  |defense      |0.1751279339938823 |\n",
      "|1  |part         |0.16022031730914288|\n",
      "|1  |well         |0.17053284421704767|\n",
      "|1  |bets         |0.27861293130724324|\n",
      "|1  |placed       |0.2284965552404658 |\n",
      "|1  |looks        |0.1973537176743789 |\n",
      "|1  |quietly      |0.25188254045524316|\n",
      "|1  |another      |0.14507889141437585|\n",
      "|1  |making       |0.1698717076460444 |\n",
      "|1  |controversial|0.20949395177306526|\n",
      "|2  |doldrums     |0.3770252270329423 |\n",
      "|2  |stocks       |0.14976769101715193|\n",
      "|2  |stock        |0.17879168082328206|\n",
      "|2  |prices       |0.14472559202114177|\n",
      "|2  |economy      |0.3721400726458204 |\n",
      "|2  |summer       |0.22694739048609625|\n",
      "|2  |plus         |0.24449073714833106|\n",
      "|2  |reuters      |0.18565512889984243|\n",
      "|2  |next         |0.14062721303262238|\n",
      "+---+-------------+-------------------+\n",
      "only showing top 50 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Join the tf and idf table together on \"word\"\n",
    "# Each word now has its term frequency and inverse document frequency\n",
    "tf_idf = tf.join(idf, on=\"word\")\n",
    "\n",
    "# Multiply the tf and idf together to get the final tf*idf score for each word in each document\n",
    "tf_idf = tf_idf.withColumn(\"tf_idf\", F.col(\"tf\") * F.col(\"idf\"))\n",
    "\n",
    "# Filtering the first few documents with id less than 5, sorting by document ID, and then showing the first 50 results\n",
    "tf_idf.filter(F.col(\"id\") < 5).select(\"id\", \"word\", \"tf_idf\").orderBy(\"id\", ascending=True).show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a92b578-d6b8-4d6f-b9f9-7cd9c52450ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:15:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "25/05/24 22:15:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "25/05/24 22:15:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/DE300_HW3/dataset/agnews_clean.csv\n",
      "                                                                                "
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/ubuntu/DE300_HW3/tfidf_output already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtf_idf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtf_idf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtfidf_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tfidf_env/lib/python3.12/site-packages/pyspark/sql/readwriter.py:2146\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   2128\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   2129\u001b[39m     compression=compression,\n\u001b[32m   2130\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2144\u001b[39m     lineSep=lineSep,\n\u001b[32m   2145\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2146\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tfidf_env/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tfidf_env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_ALREADY_EXISTS] Path file:/home/ubuntu/DE300_HW3/tfidf_output already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
     ]
    }
   ],
   "source": [
    "# Save the final results (with document ID, word, and tf*idf score) to a CSV file\n",
    "tf_idf.select(\"id\", \"word\", \"tf_idf\").write.csv(\"tfidf_output\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b518aec-3127-410c-8539-05490774b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6cb54c-5e49-43db-b376-cc0ee8f4829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 61.9M  100 61.9M    0     0   208M      0 --:--:-- --:--:-- --:--:--  209M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1391  100  1391    0     0  14588      0 --:--:-- --:--:-- --:--:-- 14642\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    22  100    22    0     0    227      0 --:--:-- --:--:-- --:--:--   229\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv\n",
    "!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv\n",
    "!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3ce15e-5f3f-4ceb-858c-42645047b563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/tfidf_env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/tfidf_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.6 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71405bcb-49c8-45e7-8889-9d9dd2971d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pands and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the main dataset\n",
    "X_y = pd.read_csv('data_for_svm.csv', header=None)\n",
    "\n",
    "# Load the weight vector dataset, reshaping it from a 2D to 1D array\n",
    "w = pd.read_csv('w.csv', header=None).values.flatten()\n",
    "\n",
    "# Load the bias dataset\n",
    "b = pd.read_csv('bias.csv', header=None).values[0][0]\n",
    "\n",
    "# Split the dataset into features X and labels Y\n",
    "# This selects all rows and columns except the last\n",
    "X = X_y.iloc[:, :-1].values\n",
    "\n",
    "# This selects all rows and the last column\n",
    "y = X_y.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2f6924-d97c-4411-86b8-a25cef8e0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_SVM(w, b, X, y, lam=0.01):\n",
    "    \"\"\"\n",
    "    Computes the soft margin SVM objective function\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the regularization term\n",
    "    reg_term = lam * np.linalg.norm(w) ** 2\n",
    "\n",
    "    # Compute hinge losses for all samples\n",
    "    margins = 1 - y * (X @ w + b)\n",
    "    hinge_losses = np.maximum(0, margins)\n",
    "\n",
    "    # Average hinge loss\n",
    "    avg_hinge = np.mean(hinge_losses)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = reg_term + avg_hinge\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a59ba146-f34b-4abc-be4a-40dba947a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Objective Loss: 0.9997559286225162\n"
     ]
    }
   ],
   "source": [
    "loss = loss_SVM(w, b, X, y)\n",
    "print(\"SVM Objective Loss:\", loss)\n",
    "\n",
    "# We can see the SVM objective loss is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d555b557-4192-4479-9115-02761379d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_SVM(w, b, X):\n",
    "    \"\"\"\n",
    "    Predict labels for input features using SVM decision rule.\n",
    "    \"\"\"\n",
    "    scores = X @ w + b\n",
    "    return np.sign(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f6e674-5eeb-4125-85e3-de671f7e59e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1.  1. -1.  1. -1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_SVM(w, b, X)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc2b46-1448-413c-90e2-4a5e76ee105d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
